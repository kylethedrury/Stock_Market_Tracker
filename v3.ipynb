{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-20T23:50:36.470185Z",
     "start_time": "2024-04-20T23:50:36.442749900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import date\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.fundamentaldata import FundamentalData\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# API_KEY = **insert key here** \n",
    "\n",
    "stocks = ['LRCX', 'MDB', 'NOW', 'SNOW', 'CRWD', 'PANW', 'META', 'SPOT', 'ASML', 'NVDA', 'TSM', 'UBER', 'OTEX', 'ADBE', 'MSFT', 'ORCL', 'PLTR', 'SNPS', 'AAPL', 'GOOG', 'IBM', 'ANET', 'AVGO', 'CRM', 'KNSL']\n",
    "print(len(stocks))\n",
    "\n",
    "stock_names = ['Lam Research Corporation', 'MongoDB Inc.', 'ServiceNow Inc.', 'Snowflake Inc.', 'CrowdStrike Holdings Inc.', 'Palo Alto Networks Inc.', 'Meta Platforms Inc.', 'Spotify Technology S.A.', 'ASML Holding N.V.', 'NVIDIA Corporation', 'Taiwan Semiconductor Manufacturing Company Limited', 'Uber Technologies Inc.', 'Open Text Corporation', 'Adobe Inc.', 'Microsoft Corporation', 'Oracle Corporation', 'Palantir Technologies Inc.', 'Synopsys Inc.', 'Apple Inc.', 'Alphabet Inc.', 'International Business Machines Corporation', 'Arista Networks Inc.', 'Broadcom Inc.', 'Salesforce.com Inc.', 'Kinsale Capital Group Inc.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Lam Research Corporation' created at './STOCKS_final/Lam Research Corporation'.\n",
      "Folder 'MongoDB Inc.' created at './STOCKS_final/MongoDB Inc.'.\n",
      "Folder 'ServiceNow Inc.' created at './STOCKS_final/ServiceNow Inc.'.\n",
      "Folder 'Snowflake Inc.' created at './STOCKS_final/Snowflake Inc.'.\n",
      "Folder 'CrowdStrike Holdings Inc.' created at './STOCKS_final/CrowdStrike Holdings Inc.'.\n",
      "Folder 'Palo Alto Networks Inc.' created at './STOCKS_final/Palo Alto Networks Inc.'.\n",
      "Folder 'Meta Platforms Inc.' created at './STOCKS_final/Meta Platforms Inc.'.\n",
      "Folder 'Spotify Technology S.A.' created at './STOCKS_final/Spotify Technology S.A.'.\n",
      "Folder 'ASML Holding N.V.' created at './STOCKS_final/ASML Holding N.V.'.\n",
      "Folder 'NVIDIA Corporation' created at './STOCKS_final/NVIDIA Corporation'.\n",
      "Folder 'Taiwan Semiconductor Manufacturing Company Limited' created at './STOCKS_final/Taiwan Semiconductor Manufacturing Company Limited'.\n",
      "Folder 'Uber Technologies Inc.' created at './STOCKS_final/Uber Technologies Inc.'.\n",
      "Folder 'Open Text Corporation' created at './STOCKS_final/Open Text Corporation'.\n",
      "Folder 'Adobe Inc.' created at './STOCKS_final/Adobe Inc.'.\n",
      "Folder 'Microsoft Corporation' created at './STOCKS_final/Microsoft Corporation'.\n",
      "Folder 'Oracle Corporation' created at './STOCKS_final/Oracle Corporation'.\n",
      "Folder 'Palantir Technologies Inc.' created at './STOCKS_final/Palantir Technologies Inc.'.\n",
      "Folder 'Synopsys Inc.' created at './STOCKS_final/Synopsys Inc.'.\n",
      "Folder 'Apple Inc.' created at './STOCKS_final/Apple Inc.'.\n",
      "Folder 'Alphabet Inc.' created at './STOCKS_final/Alphabet Inc.'.\n",
      "Folder 'International Business Machines Corporation' created at './STOCKS_final/International Business Machines Corporation'.\n",
      "Folder 'Arista Networks Inc.' created at './STOCKS_final/Arista Networks Inc.'.\n",
      "Folder 'Broadcom Inc.' created at './STOCKS_final/Broadcom Inc.'.\n",
      "Folder 'Salesforce.com Inc.' created at './STOCKS_final/Salesforce.com Inc.'.\n",
      "Folder 'Kinsale Capital Group Inc.' created at './STOCKS_final/Kinsale Capital Group Inc.'.\n"
     ]
    }
   ],
   "source": [
    "def create_folders(parent_dir, folder_names):              # for creating directory structure\n",
    "    for name in folder_names:\n",
    "        # Construct the full path of the new folder\n",
    "        folder_path = os.path.join(parent_dir, name)\n",
    "\n",
    "        # Check if folder already exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Folder '{name}' created at '{folder_path}'.\")\n",
    "        else:\n",
    "            print(f\"Folder '{name}' already exists at '{folder_path}'.\")\n",
    "\n",
    "#create_folders('./STOCKS_final/', stock_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T23:50:37.666794900Z",
     "start_time": "2024-04-20T23:50:37.613776500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for LRCX\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for MDB\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for NOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for SNOW\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for CRWD\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for PANW\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for META\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for SPOT\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for ASML\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for NVDA\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for TSM\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for UBER\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for OTEX\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for ADBE\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for MSFT\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for ORCL\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for PLTR\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for SNPS\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for AAPL\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for GOOG\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for IBM\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for ANET\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for AVGO\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for CRM\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n",
      "CSV file created for KNSL\n"
     ]
    }
   ],
   "source": [
    "def create_csv_with_header(filename):           # for creating csv files\n",
    "    # Define the header row\n",
    "    header = [\n",
    "        'price', 'price_change', 'price_change_percent', 'market_cap', 'PE', 'dividend_yield', 'eps',\n",
    "        'week_high', 'week_low', 'beta', 'forward_pe', 'price_to_sales', 'price_to_book',\n",
    "        'earnings_yield', 'latest_volume', 'average_volume', 'analyst_recommendation'\n",
    "    ]\n",
    "\n",
    "    # Write the header to the CSV file\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "\n",
    "def main(folder, symbol):                  # Iterate over each stock symbol in a sector\n",
    "    for file in folder:\n",
    "        # Create a CSV file for the stock symbol\n",
    "        filename = f\"./STOCKS_final/{symbol}.csv\"\n",
    "        create_csv_with_header(filename)\n",
    "        print(f\"CSV file created for {symbol}\")\n",
    "\n",
    "#for i in range(len(stocks)):\n",
    "    #main('./STOCKS_final', stocks[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T00:05:40.773519500Z",
     "start_time": "2024-04-21T00:05:40.729662900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def save_data(symbol):\n",
    "    ts = TimeSeries(key=API_KEY)\n",
    "    fd = FundamentalData(key=API_KEY)\n",
    "\n",
    "    # Get quote data\n",
    "    data, meta_data = ts.get_quote_endpoint(symbol=symbol)\n",
    "\n",
    "    if 'Error Message' in data:\n",
    "        print(\"Error:\", data['Error Message'])\n",
    "        return None\n",
    "\n",
    "    # Extracting data points\n",
    "    price = float(data.get('05. price'))\n",
    "    price_change = float(data.get('09. change'))\n",
    "    price_change_percent = price_change / price * 100\n",
    "\n",
    "    # Get company overview data\n",
    "    data, meta_data = fd.get_company_overview(symbol=symbol)\n",
    "\n",
    "    if 'Error Message' in data:\n",
    "        print(\"Error:\", data['Error Message'])\n",
    "        return None\n",
    "\n",
    "    market_cap = float(data.get('MarketCapitalization'))\n",
    "\n",
    "    if data['PERatio'] == 'None': # Get the P/E ratio\n",
    "        PE = 0\n",
    "    else:                         # in case ratio not available\n",
    "        PE = float(data['PERatio'])\n",
    "\n",
    "    if data['DividendYield'] == 'None': # Get the P/E ratio\n",
    "        dividend_yield = 0\n",
    "    else:                         # in case ratio not available\n",
    "        dividend_yield = float(data['DividendYield'])\n",
    "\n",
    "    eps = float(data.get('EPS'))\n",
    "    week_high = float(data.get('52WeekHigh'))\n",
    "    week_low = float(data.get('52WeekLow', \"N/A\"))  # Provide a default value if key doesn't exist\n",
    "    beta = float(data.get('Beta'))\n",
    "    forward_pe = float(data.get('ForwardPE'))\n",
    "    price_to_sales = float(data.get('PriceToSalesRatioTTM'))\n",
    "\n",
    "    if data['PriceToBookRatio'] == 'None' or '-': # Get the P/E ratio\n",
    "        price_to_book = 0\n",
    "    else:                         # in case ratio not available\n",
    "        price_to_book = float(data['PriceToBookRatio'])\n",
    "\n",
    "    earnings_yield = eps / price * 100\n",
    "    analyst_recommendation = data.get('AnalystRecommendation')\n",
    "\n",
    "    # Retrieve volume and average volume data\n",
    "    volume_data, _ = ts.get_daily(symbol=symbol)\n",
    "    volumes = [int(volume_data[date]['5. volume']) for date in volume_data]\n",
    "    average_volume = sum(volumes) / len(volumes)\n",
    "    latest_volume = int(volume_data[max(volume_data.keys())]['5. volume'])\n",
    "\n",
    "    # Saving data points into an array\n",
    "    data_points = [\n",
    "        price, price_change, price_change_percent, market_cap, PE, dividend_yield, eps,\n",
    "        week_high, week_low, beta, forward_pe, price_to_sales, price_to_book,\n",
    "        earnings_yield, latest_volume, average_volume, analyst_recommendation,\n",
    "    ]\n",
    "\n",
    "    print(\"Data points saved to array:\", data_points)\n",
    "\n",
    "    return data_points\n",
    "\n",
    "def save_data_to_csv(symbol):\n",
    "    filename = f\"./STOCKS_final/{symbol}.csv\"\n",
    "\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(save_data(symbol))\n",
    "\n",
    "    print(f\"Data points saved to {filename}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T00:10:31.877695800Z",
     "start_time": "2024-04-21T00:10:31.855768800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points saved to array: [870.25, -18.67, -2.1453605285837405, 114092384000.0, 33.57, 0.0092, 25.92, 1005.23, 488.2, 1.501, 26.25, 8.76, 0, 2.9784544671071536, 1354476, 1022390.22, None]\n",
      "Data points saved to ./STOCKS_final/LRCX.csv\n",
      "Data points saved to array: [327.47, -8.08, -2.4674015940391483, 23850002000.0, 0, 0, -2.48, 509.62, 212.52, 1.21, 142.86, 15.24, 0, -0.7573212813387485, 1353402, 1564457.12, None]\n",
      "Data points saved to ./STOCKS_final/MDB.csv\n",
      "Data points saved to array: [713.91, -17.45, -2.4442856942751887, 146624266000.0, 84.79, 0, 8.42, 815.32, 427.68, 0.957, 58.14, 17.6, 0, 1.1794203751173118, 1540609, 1156733.28, None]\n",
      "Data points saved to ./STOCKS_final/NOW.csv\n",
      "Data points saved to array: [145.45, -2.96, -2.035063595737367, 48609391000.0, 0, 0, -2.55, 237.72, 135.26, 0.907, 212.77, 18.88, 0, -1.7531797868683399, 4974560, 6470137.6, None]\n",
      "Data points saved to ./STOCKS_final/SNOW.csv\n",
      "Data points saved to array: [282.64, -11.46, -4.054627795075008, 68361855000.0, 763.89, 0, 0.37, 365.0, 115.67, 1.052, 78.12, 24.46, 0, 0.1309085762807812, 3116556, 3609122.08, None]\n",
      "Data points saved to ./STOCKS_final/CRWD.csv\n",
      "Data points saved to array: [277.71, -3.43, -1.235101364732995, 89728098000.0, 42.92, 0, 6.47, 380.84, 176.3, 1.2, 45.05, 11.98, 0, 2.3297684635050953, 4751615, 5265055.99, None]\n",
      "Data points saved to ./STOCKS_final/PANW.csv\n",
      "Data points saved to array: [481.07, -20.73, -4.309144199388863, 1220248470000.0, 32.37, 0.0042, 14.86, 531.49, 206.91, 1.184, 25.32, 9.67, 0, 3.088947554409961, 25215364, 17039967.12, None]\n",
      "Data points saved to ./STOCKS_final/META.csv\n",
      "Data points saved to array: [275.83, -13.37, -4.847188485661459, 54704538000.0, 0, 0, -2.92, 313.16, 128.67, 1.646, 96.15, 4.498, 0, -1.0586230649312982, 4245510, 1895167.69, None]\n",
      "Data points saved to ./STOCKS_final/SPOT.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Thank you for using Alpha Vantage! Our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(stocks)):\n\u001B[1;32m----> 2\u001B[0m     \u001B[43msave_data_to_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstocks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 73\u001B[0m, in \u001B[0;36msave_data_to_csv\u001B[1;34m(symbol)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m, newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m csvfile:\n\u001B[0;32m     72\u001B[0m     writer \u001B[38;5;241m=\u001B[39m csv\u001B[38;5;241m.\u001B[39mwriter(csvfile)\n\u001B[1;32m---> 73\u001B[0m     writer\u001B[38;5;241m.\u001B[39mwriterow(\u001B[43msave_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43msymbol\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData points saved to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 52\u001B[0m, in \u001B[0;36msave_data\u001B[1;34m(symbol)\u001B[0m\n\u001B[0;32m     49\u001B[0m analyst_recommendation \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAnalystRecommendation\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Retrieve volume and average volume data\u001B[39;00m\n\u001B[1;32m---> 52\u001B[0m volume_data, _ \u001B[38;5;241m=\u001B[39m \u001B[43mts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_daily\u001B[49m\u001B[43m(\u001B[49m\u001B[43msymbol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msymbol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m volumes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(volume_data[date][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m5. volume\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m date \u001B[38;5;129;01min\u001B[39;00m volume_data]\n\u001B[0;32m     54\u001B[0m average_volume \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(volumes) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(volumes)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Stock_API\\lib\\site-packages\\alpha_vantage\\alphavantage.py:218\u001B[0m, in \u001B[0;36mAlphaVantage._output_format.<locals>._format_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_format_wrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 218\u001B[0m     call_response, data_key, meta_data_key \u001B[38;5;241m=\u001B[39m func(\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_format\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpandas\u001B[39m\u001B[38;5;124m'\u001B[39m \\\n\u001B[0;32m    221\u001B[0m             \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_format\u001B[38;5;241m.\u001B[39mlower():\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Stock_API\\lib\\site-packages\\alpha_vantage\\alphavantage.py:160\u001B[0m, in \u001B[0;36mAlphaVantage._call_api_on_func.<locals>._call_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    159\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(url, apikey_parameter)\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m, data_key, meta_data_key\n",
      "File \u001B[1;32m~\\PycharmProjects\\Stock_API\\lib\\site-packages\\alpha_vantage\\alphavantage.py:361\u001B[0m, in \u001B[0;36mAlphaVantage._handle_api_call\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(json_response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError Message\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInformation\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m json_response \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtreat_info_as_error:\n\u001B[1;32m--> 361\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(json_response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInformation\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNote\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m json_response \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtreat_info_as_error:\n\u001B[0;32m    363\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(json_response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNote\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[1;31mValueError\u001B[0m: Thank you for using Alpha Vantage! Our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits."
     ]
    }
   ],
   "source": [
    "for i in range(len(stocks)):\n",
    "    save_data_to_csv(stocks[i])\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T00:11:00.308322600Z",
     "start_time": "2024-04-21T00:10:52.174514600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
